{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informal-couple",
   "metadata": {},
   "source": [
    "# Ames Housing DataSet\n",
    "\n",
    "* The Ames Housing dataset was compiled by Dean De Cock for use in data science education. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this dataset can be used to predict the final price of each home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-patio",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(file_path):\n",
    "    HOUSING_PATH = 'data/'\n",
    "    csv_path = os.path.join(HOUSING_PATH, file_path)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-madagascar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = load_housing_data('train.csv')\n",
    "test = load_housing_data('test.csv')\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of each feature\n",
    "# nominal : categorical features with no clear ordering\n",
    "# ordinal : has an ordering, ie. 0-10, A-Z scale\n",
    "\n",
    "cat_feats_nominal = ['MSSubClass', 'MSZoning', 'Neighborhood', 'Condition1', 'Condition2', 'HouseStyle', 'CentralAir', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Electrical', 'MasVnrType', 'Exterior1st', 'Exterior2nd', 'Heating', 'Foundation']\n",
    "cat_feats_ordinal = ['Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'BldgType', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','PavedDrive', 'Fence']\n",
    "\n",
    "numeric_feats_cont= ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'TotalSF', 'TotalSF1', 'YrBltAndRemod', 'TotalBathrooms', 'TotalPorchSF']\n",
    "numeric_feats_ordinal= ['OverallQual', 'OverallCond']\n",
    "numeric_feats_discrete= ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars','haspool', 'has2ndfloor', 'hasgarage', 'hasbsmt', 'hasfireplace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-webmaster",
   "metadata": {},
   "source": [
    "### Drop unnecessary ID attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Id', inplace= True, axis=1)\n",
    "test.drop('Id', inplace= True, axis=1)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-arrival",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-promise",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure = train.hist(bins=50, figsize=(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-vertex",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "train_corr = train.corr(method= 'pearson') # Compute pairwise correlation of columns, excluding NA/null values. pearson : standard correlation coefficient\n",
    "f, ax = plt.subplots(figsize=(25, 25))\n",
    "\n",
    "# Generate a mask to hide  the upper triangle\n",
    "mask = np.triu(np.ones_like(train_corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "ax = sns.heatmap(train_corr, vmin=-1, vmax=1, mask=mask, cmap=cmap, center=0, annot = True, square=True, linewidths=.5, cbar_kws= {\"shrink\": .5, 'orientation': 'vertical'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-memorabilia",
   "metadata": {},
   "source": [
    "The features that highly correlate with `SalePrice` are: \n",
    "- `OverallQual` (0.79)\n",
    "- `GrLivArea` (0.71), `GarageArea` (0.62), `TotalBsmtSF` (0.61), `1stFlrSF` (0.61)\n",
    "- `YearBuilt` (0.52), `YearRemodAdd` (0.51)\n",
    "- `FullBath`(0.56), `TotRmsAbvGrd` (0.53), `GarageCars` (0.64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-prayer",
   "metadata": {},
   "source": [
    "Let's take a loot at these attributes separately with respect to `SalePrice`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-security",
   "metadata": {},
   "source": [
    "### `OveralQual` vs. `SalePrice`\n",
    "\n",
    "* Correlation coefficient is 0.79\n",
    "* Since `OverallQual` rates the overall material and finish and ranges from 10 (Very Excellent) to 1 (Very Poor), we can use boxplot and violinplot for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-mileage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1,2, figsize = (24,10))\n",
    "sns.boxplot(data=train, x= 'OverallQual', y='SalePrice', ax = ax[0])\n",
    "sns.violinplot(data=train, x= 'OverallQual', y='SalePrice', ax = ax[1])\n",
    "ax[0].set_title('OverallQual vs. SalePrice (Correlation coefficient: 0.79)')\n",
    "ax[1].set_title('OverallQual vs. SalePrice (Correlation coefficient: 0.79)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-somalia",
   "metadata": {},
   "source": [
    "### `TotalBsmtSF`, `1stFlrSF`, `GrLivArea`, `GarageArea` vs. `SalePrice`\n",
    "\n",
    "* Correlation coefficients are `TotalBsmtSF` (0.61), `1stFlrSF` (0.61), `GrLivArea` (0.71), `GarageArea` (0.62) \n",
    "* Since all of these features contains surface areas in square feet, we can use regplots to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(2,2, figsize = (24,8))\n",
    "figure.tight_layout(pad=5.0)\n",
    "\n",
    "sns.regplot(data=train, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[0,0])\n",
    "ax[0,0].set_title('TotalBsmtSF vs SalePrice (Correlation coefficient: 0.61)', fontsize = 12)\n",
    "\n",
    "sns.regplot(data=train, x = '1stFlrSF', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[0,1])\n",
    "ax[0,1].set_title('1stFlrSF vs SalePrice (Correlation coefficient: 0.61)', fontsize = 12)\n",
    "\n",
    "sns.regplot(data=train, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[1,0])\n",
    "ax[1,0].set_title('\\nGrLivArea vs SalePrice (Correlation coefficient: 0.71)', fontsize = 12)\n",
    "\n",
    "sns.regplot(data=train, x = 'GarageArea', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[1,1])\n",
    "ax[1,1].set_title('\\nGarageArea vs SalePrice (Correlation coefficient: 0.62)', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-communication",
   "metadata": {},
   "source": [
    "### `YearBuilt`, `YearRemodAd` vs. `SalePrice`\n",
    "* Correlation coefficients are 0.52 and 0.51 respectively\n",
    "* Since these contains year values, we can use the barplot and regplot to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(2,1, figsize = (24,10))\n",
    "figure.tight_layout(pad=5.0) # To increase the space between subplots\n",
    "\n",
    "sns.barplot(ax = ax[0], x='YearBuilt', y=\"SalePrice\", data = train)\n",
    "ax[0].set(xlabel=\"YearBuilt\", ylabel = \"SalePrice\")\n",
    "ax[0].set_title('YearBuilt vs SalePrice (Correlation coefficient: 0.52)')\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(),rotation=90)\n",
    "\n",
    "sns.barplot(ax = ax[1], x='YearRemodAdd', y=\"SalePrice\", data = train)\n",
    "ax[1].set(xlabel=\"YearRemodAdd\", ylabel = \"SalePrice\")\n",
    "ax[1].set_title('YearRemodAdd vs SalePrice (Correlation coefficient: 0.51)')\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-craft",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1,2, figsize = (24,6))\n",
    "sns.regplot(data=train, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[0]) # scatter_kws and line_kws used to pass additional keyword argument to change transparancy and line color\n",
    "ax[0].set_title('YearBuilt vs SalePrice (Correlation coefficient: 0.52)', fontsize = 12)\n",
    "\n",
    "sns.regplot(data=train, x = 'YearRemodAdd', y='SalePrice', scatter_kws={'alpha':0.2}, line_kws={'color': 'blue'}, ax = ax[1])\n",
    "ax[1].set_title('YearRemodAdd vs SalePrice (Correlation coefficient: 0.51)', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-hours",
   "metadata": {},
   "source": [
    "### `FullBath`, `TotRmsAbvGrd`, `GarageCars` vs. `SalePrice`\n",
    "\n",
    "-  Correlation coefficients are `FullBath`(0.56), `TotRmsAbvGrd` (0.53), `GarageCars` (0.64)\n",
    "- These features contain discrete values so we will use boxplot and violinplots for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(3,2, figsize = (24,12))\n",
    "figure.tight_layout(pad=4.0) # To increase the space between subplots\n",
    "\n",
    "sns.boxplot(data=train, x = 'FullBath', y='SalePrice', ax = ax[0,0])\n",
    "sns.violinplot(data=train, x = 'FullBath', y='SalePrice', ax = ax[0,1])\n",
    "ax[0,0].set_title('FullBath vs SalePrice (Correlation coefficient: 0.56)', fontsize = 12)\n",
    "ax[0,1].set_title('FullBath vs SalePrice (Correlation coefficient: 0.56)', fontsize = 12)\n",
    "\n",
    "sns.boxplot(data=train, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1,0])\n",
    "sns.violinplot(data=train, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1,1])\n",
    "ax[1,0].set_title('TotRmsAbvGrd vs SalePrice (Correlation coefficient: 0.53)', fontsize = 12)\n",
    "ax[1,1].set_title('TotRmsAbvGrd vs SalePrice (Correlation coefficient: 0.53)', fontsize = 12)\n",
    "\n",
    "sns.boxplot(data=train, x = 'GarageCars', y='SalePrice', ax = ax[2,0])\n",
    "sns.violinplot(data=train, x = 'GarageCars', y='SalePrice', ax = ax[2,1])\n",
    "ax[2,0].set_title('GarageCars vs SalePrice (Correlation coefficient: 0.64)', fontsize = 12)\n",
    "ax[2,1].set_title('GarageCars vs SalePrice (Correlation coefficient: 0.64)', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-opening",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "* We will remove outliers outside of the minimum and maximum threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_percentile= 0.001\n",
    "max_percentile= 0.999\n",
    "# Use numeric features\n",
    "features = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n",
    "            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n",
    "            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n",
    "target= 'SalePrice'\n",
    "nrows= int(np.ceil(len(features)/2))\n",
    "ncols= 2 \n",
    "\n",
    "def remove_outliers(inline_delete=True):\n",
    "    global train\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=((24,nrows * 6)))\n",
    "    outliers = []\n",
    "    cnt = 0\n",
    "    for row in range(0, nrows):\n",
    "        for col in range(0, ncols):\n",
    "            min_threshold, max_threshold = train[features[cnt]].quantile([min_percentile, max_percentile])\n",
    "            df_outliers = train[(train[features[cnt]] < min_threshold) | (train[features[cnt]] > max_threshold)]\n",
    "            outliers += df_outliers.index.tolist()\n",
    "            \n",
    "            ax[row][col].scatter(x=train[features[cnt]], y=train[target])\n",
    "            \n",
    "            ax[row][col].scatter(x= df_outliers[features[cnt]],  y=df_outliers[target], marker =\"o\", edgecolor =\"red\", s = 100)\n",
    "            ax[row][col].set_xlabel(features[cnt])\n",
    "            ax[row][col].set_ylabel(target)\n",
    "            ax[row][col].set_title('Outlier detection for feature ' + features[cnt])\n",
    "            \n",
    "            if inline_delete:\n",
    "                train = train.drop(df_outliers.index.tolist())\n",
    "                train.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "            cnt += 1\n",
    "            if cnt >= len(features):\n",
    "                break\n",
    "                    \n",
    "            \n",
    "    plt.show()\n",
    "                \n",
    "    unique_outliers= list(set(outliers))\n",
    "                              \n",
    "    if not inline_delete:\n",
    "        train = train.drop(unique_outliers)\n",
    "        train.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "remove_outliers(inline_delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-trial",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-trail",
   "metadata": {},
   "source": [
    "### Separate target feature (`SalePrice`) from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.SalePrice\n",
    "train.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-diagram",
   "metadata": {},
   "source": [
    "### Delete Unnecessary Features\n",
    "\n",
    "* Drop `Utilities`, `Street`, and `PoolQC` features\n",
    "* `Utilities` contains only one type of utility and is not useful for the model\n",
    "* `Street` contains unbalanced data of type road access\n",
    "* `PoolQC` has mostly missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Utilities', 'Street', 'PoolQC'], axis=1, inplace=True)\n",
    "test.drop(['Utilities', 'Street', 'PoolQC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-condition",
   "metadata": {},
   "source": [
    "### Fix DataTypes\n",
    "* `MSSubClass`: Identifies the type of dwelling involved in the sale. Its data type is int64 and values are incremental order starting from 20 upto 190. If we keep it as it is then our model may give more importance to MSSubClass 190 houses over MSSubClass 20 houses. In order to avoid that we will change its data type to 'str' and treat this as categorical variable.\n",
    "* `YrSold`: Contains year values like 2008, 2007, 2006, 2009, 2010. Since we have sufficient data for each value we will change its data type to 'str' and treat this as categorical variable.\n",
    "* `MoSold`: Since we have sufficient data for each value we will change its data type to 'str' and treat this as categorical variable.\n",
    "* We are not changing the data type of `YearBuilt`, `YearRemodAdd` and `GarageYrBlt` to 'str'. They have linear relationship with `SalePrice`, model will benefit from this relationship instaed of converting it into categories. (Also we dont have sufficient training examples for each unique value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('MSSubClass', 'YrSold', 'MoSold'):\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-vietnam",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_na = train.columns[train.isnull().any()]\n",
    "test_col_na = test.columns[test.isnull().any()]\n",
    "\n",
    "# Get missing value count in each column\n",
    "train_na_cnt = train[train_col_na].isnull().sum()\n",
    "test_na_cnt = test[test_col_na].isnull().sum()\n",
    "\n",
    "# Get missing values percentage for each column\n",
    "train_na = (train[train_col_na].isnull().sum()/len(train)) * 100\n",
    "test_na = (test[test_col_na].isnull().sum()/len(test)) * 100\n",
    "\n",
    "\n",
    "display_na = pd.DataFrame({'Train Null Val': train_na_cnt, 'Train Null Val %': train_na, 'Test Null Val': test_na_cnt, 'Test Null Val %': test_na})\n",
    "display_na = display_na.sort_values(by='Train Null Val %', ascending=False)\n",
    "display_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=display_na.index, y='Train Null Val %', data=display_na)\n",
    "plt.xticks(rotation = 90) \n",
    "plt.title('Train Column name with null value %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-salem",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=display_na.index, y='Test Null Val %', data=display_na)\n",
    "plt.xticks(rotation = 90) \n",
    "plt.title('Test Column name with null value %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-producer",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-breath",
   "metadata": {},
   "source": [
    "### Replace Missing Categorical Features with 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('MiscFeature', 'Alley','FireplaceQu', 'GarageFinish', 'GarageQual', 'Fence', 'GarageType', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n",
    "    train[col] = train[col].fillna('None')\n",
    "    test[col] = test[col].fillna('None')\n",
    "\n",
    "    print(f'Feature: {col}, Null Count: {train[col].isnull().sum()}, Unique Values: {train[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-power",
   "metadata": {},
   "source": [
    "### Replace with Median\n",
    "\n",
    "* `LotFrontage`: Linear feet of street connected to property. Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood, we can fill in missing values by the median LotFrontage of the neighborhood.\n",
    "\n",
    "* Since `LotFrontage` contains continuous data we are taking 'median' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-baseline",
   "metadata": {},
   "source": [
    "### Replace Missing Numerical Values with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    train[col] = train[col].fillna(0)\n",
    "    test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-charter",
   "metadata": {},
   "source": [
    "### Impute with Specific Value\n",
    "\n",
    "* As per the data description, assume 'Typ' home functionality unless deductions are warranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Functional'] = train['Functional'].fillna('Typ')\n",
    "test['Functional'] = test['Functional'].fillna('Typ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-cutting",
   "metadata": {},
   "source": [
    "### Replace with Mode\n",
    "\n",
    "* For features with low percentage of null values, we will use the most frequent value to replace the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd', 'SaleType'):\n",
    "    train[col] = train[col].fillna(train[col].mode()[0])\n",
    "    test[col] = test[col].fillna(test[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-morris",
   "metadata": {},
   "source": [
    "One last check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-singing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Count of train null values: {train.isnull().sum().sum()}')\n",
    "print(f'Count of test null values: {test.isnull().sum().sum()}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 3))\n",
    "sns.heatmap(train.isnull(), ax=ax[0])\n",
    "sns.heatmap(test.isnull(), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-litigation",
   "metadata": {},
   "source": [
    "## Remove Multicollinear Features\n",
    "\n",
    "* Method `get_highest_vif_feature()` will find all the features with Variance Inflation Factor (VIF) more than threshold value and return the feature with highest VIF\n",
    "* Instead of removing all the features with VIF more than threshold value we will drop the feature with highest VIF value and recalculate the VIF for all the remaining features.\n",
    "* We will repeat this step until no remaining features have a VIF larger than threshold value.\n",
    "* Once we get the final list of multicolenear features we will verify it with Correlation Matrix results and only drop those features, which we are 100% sure won't help in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def get_highest_vif_feature(df, thresh=5):\n",
    "    '''\n",
    "    Ref: https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python\n",
    "    \n",
    "    Calculates VIF each feature in a pandas dataframe\n",
    "    A constant must be added to variance_inflation_factor or the results will be incorrect\n",
    "\n",
    "    :param df: the pandas dataframe containing only the predictor features, not the response variable\n",
    "    :param thresh: the max VIF value before the feature is removed from the dataframe\n",
    "    :return: dataframe with features removed\n",
    "    '''\n",
    "   \n",
    "    const = add_constant(df)\n",
    "    print(f'Shape of data after adding const column: {const.shape}')\n",
    "    cols = const.columns\n",
    "    \n",
    "    # Calculating VIF for each feature\n",
    "    vif_df = pd.Series([ (variance_inflation_factor(const.values, i)) for i in range(const.shape[1]) ], index= const.columns).to_frame()\n",
    "    \n",
    "    vif_df = vif_df.sort_values(by=0, ascending=False).rename(columns={0: 'VIF'})\n",
    "    vif_df = vif_df.drop('const')\n",
    "    vif_df = vif_df[vif_df['VIF'] > thresh]\n",
    "\n",
    "    if vif_df.empty:\n",
    "        print('DataFrame is empty!')\n",
    "        return None\n",
    "    else:\n",
    "        print(f'\\nFeatures above VIF threshold: {vif_df.to_dict()}')       \n",
    "        # Feature with highest VIF value\n",
    "        return list(vif_df.index)[0]\n",
    "        print(f'Lets delete the feature with highest VIF value: {list(vif_df.index)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numeric features\n",
    "print(f'Shape of input data: {train.shape}')\n",
    "numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "print(f\"Calculating VIF for {len(numeric_feats)} numerical features\")\n",
    "\n",
    "df_numeric = train[numeric_feats]\n",
    "print(f'Shape of df_numeric: {df_numeric.shape}')\n",
    "    \n",
    "feature_to_drop = None\n",
    "feature_to_drop_list = []\n",
    "while True:\n",
    "    feature_to_drop = get_highest_vif_feature(df_numeric, thresh=5)\n",
    "    print(f'feature_to_drop: {feature_to_drop}')\n",
    "    if feature_to_drop is None:\n",
    "        print('No more features to drop!')\n",
    "        break\n",
    "    else:\n",
    "        feature_to_drop_list.append(feature_to_drop)\n",
    "        df_numeric = df_numeric.drop(feature_to_drop, axis=1)\n",
    "        print(f'Feature {feature_to_drop} droped from df_numeric')\n",
    "\n",
    "print(f'\\nfeature_to_drop_list: {feature_to_drop_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numeric features\n",
    "print(f'Shape of input data: {test.shape}')\n",
    "numeric_feats = test.dtypes[test.dtypes != \"object\"].index\n",
    "print(f\"Calculating VIF for {len(numeric_feats)} numerical features\")\n",
    "\n",
    "df_numeric = test[numeric_feats]\n",
    "print(f'Shape of df_numeric: {df_numeric.shape}')\n",
    "    \n",
    "feature_to_drop = None\n",
    "feature_to_drop_list = []\n",
    "while True:\n",
    "    feature_to_drop = get_highest_vif_feature(df_numeric, thresh=5)\n",
    "    print(f'feature_to_drop: {feature_to_drop}')\n",
    "    if feature_to_drop is None:\n",
    "        print('No more features to drop!')\n",
    "        break\n",
    "    else:\n",
    "        feature_to_drop_list.append(feature_to_drop)\n",
    "        df_numeric = df_numeric.drop(feature_to_drop, axis=1)\n",
    "        print(f'Feature {feature_to_drop} droped from df_numeric')\n",
    "\n",
    "print(f'\\nfeature_to_drop_list: {feature_to_drop_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-forum",
   "metadata": {},
   "source": [
    "Now lets compare the result of VIF with correlation matrix values.\n",
    "\n",
    "* Features correlated with `SalePrice` are OverallQual(0.79), YearBuilt(0.52), YearRemodAdd(0.51), TotalBsmtSF(0.61), 1stFlrSF(0.61), GrLivArea(0.71), FullBath(0.56), TotRmsAbvGrd(0.53), GarageCars(0.64), GarageArea(0.62)\n",
    "* Features not correlated with SalePrice are MSSubClass(-0.084), OverallCond(-0.078), BsmtFinSF1(-0.011), LowQualFinSF(-0.026), BsmtHalfBath(-0.017), KitchenAbvGrd(-0.14), EnclosedPorch(-0.13), MiscVal(-0.021), YrSold(-0.029)\n",
    "* Even though area features are correlated, we dont want to delete them. These features are very usefull to predict the sales price. Infact next during feature engineering we are going to add few more area features by combining existing features. Now from the list of features predicted by VIF we can only delete `LowQualFinSF` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-apparel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['LowQualFinSF'], axis=1)\n",
    "test = test.drop(['LowQualFinSF'], axis=1)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-collar",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Let's check the skewness of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "cat_feats = train.dtypes[train.dtypes == \"object\"].index\n",
    "numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "print(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n",
    "\n",
    "train_skew_features = train[numeric_feats].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n",
    "train_skewness = pd.DataFrame({'Skew': train_skew_features})\n",
    "\n",
    "print(f'Skew in numerical features. Shape of skewness: {train_skewness.shape}')\n",
    "train_skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = test.dtypes[test.dtypes == \"object\"].index\n",
    "numeric_feats = test.dtypes[test.dtypes != \"object\"].index\n",
    "print(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n",
    "\n",
    "test_skew_features = test[numeric_feats].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n",
    "test_skewness = pd.DataFrame({'Skew': test_skew_features})\n",
    "\n",
    "print(f'Skew in numerical features. Shape of skewness: {test_skewness.shape}')\n",
    "test_skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-arena",
   "metadata": {},
   "source": [
    "* As we can see from above details that our data is skewed. Remember that Multivariate normality is one of the assumption for linear regression. Multivariate normality means that regression requires all its variables to be normal. By having skewed data we violate the assumption of normality.\n",
    "* So we must choose the right method which will scale our features as well as maintain the multivariate normality.\n",
    "* We will use a Box Cox transformation. A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "\n",
    "train_high_skew = train_skew_features[train_skew_features > 0.5]\n",
    "test_high_skew = test_skew_features[test_skew_features > 0.5]\n",
    "\n",
    "train_skew_index = train_high_skew.index\n",
    "test_skew_index = test_high_skew.index\n",
    "\n",
    "for i in train_skew_index:\n",
    "    train[i] = boxcox1p(train[i], stats.boxcox_normmax(train[i] + 1))\n",
    "for j in test_skew_index:\n",
    "    test[i] = boxcox1p(test[i], stats.boxcox_normmax(test[i] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-kitchen",
   "metadata": {},
   "source": [
    "## Features Engineering\n",
    "\n",
    "* Since area related features are very important to determine the house price, we will create new feature by name 'TotalSF' by adding 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'.\n",
    "* Similarly we will create one more new feature by name 'TotalSF1' by adding 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'. Here 'BsmtFinSF1' and 'BsmtFinSF2' represent finished square feet of all area, thats why we are creating separate feature using it.\n",
    "* Create new feature 'YrBltAndRemod' by adding 'YearBuilt' and 'YearRemodAdd'\n",
    "* Create new feature 'TotalBathrooms' by adding all the bathrooms in the house.\n",
    "* Create new feature 'TotalPorchSF' by adding all porch area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "train['TotalSF1'] = train['BsmtFinSF1'] + train['BsmtFinSF2'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "\n",
    "train['YrBltAndRemod']= train['YearBuilt'] + train['YearRemodAdd']\n",
    "\n",
    "train['TotalBathrooms'] = (train['FullBath'] + (0.5 * train['HalfBath']) +\n",
    "                               train['BsmtFullBath'] + (0.5 * train['BsmtHalfBath']))\n",
    "\n",
    "train['TotalPorchSF'] = (train['OpenPorchSF'] + train['3SsnPorch'] +\n",
    "                              train['EnclosedPorch'] + train['ScreenPorch'] +\n",
    "                              train['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']\n",
    "test['TotalSF1'] = test['BsmtFinSF1'] + test['BsmtFinSF2'] + test['1stFlrSF'] + test['2ndFlrSF']\n",
    "\n",
    "test['YrBltAndRemod']= test['YearBuilt'] + test['YearRemodAdd']\n",
    "\n",
    "test['TotalBathrooms'] = (test['FullBath'] + (0.5 * test['HalfBath']) +\n",
    "                               test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath']))\n",
    "\n",
    "test['TotalPorchSF'] = (test['OpenPorchSF'] + test['3SsnPorch'] +\n",
    "                              test['EnclosedPorch'] + test['ScreenPorch'] +\n",
    "                              test['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-threat",
   "metadata": {},
   "source": [
    "Let's add new features based on the availability of the swimming pool, second floor, garage, basement and fireplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['haspool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "train['has2ndfloor'] = train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "train['hasgarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "train['hasbsmt'] = train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "train['hasfireplace'] = train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['haspool'] = test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "test['has2ndfloor'] = test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "test['hasgarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "test['hasbsmt'] = test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "test['hasfireplace'] = test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-female",
   "metadata": {},
   "source": [
    "## Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually label cat ordinal features\n",
    "s = [train, test]\n",
    "\n",
    "for dataset in s:\n",
    "    dataset['Alley'].replace(to_replace = ['None', 'Grvl', 'Pave'], value = [0, 1, 2], inplace = True)\n",
    "    dataset['LotShape'].replace(to_replace = ['Reg', 'IR1', 'IR2', 'IR3'], value = [3, 2, 1,0], inplace = True)\n",
    "    dataset['LandContour'].replace(to_replace = ['Lvl', 'Bnk', 'Low', 'HLS'], value = [3, 2, 1,0], inplace = True)\n",
    "    dataset['LotConfig'].replace(to_replace = ['Inside', 'FR2', 'Corner', 'CulDSac', 'FR3'], value = [0, 3, 1, 2, 4], inplace = True)\n",
    "    dataset['LandSlope'].replace(to_replace = ['Gtl', 'Mod', 'Sev'], value = [2, 1, 0], inplace = True)\n",
    "    dataset['BldgType'].replace(to_replace = ['1Fam', '2fmCon', 'Duplex', 'TwnhsE', 'Twnhs'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['RoofStyle'].replace(to_replace = ['Gable', 'Hip', 'Gambrel', 'Mansard', 'Flat', 'Shed'], value = [4, 2, 3, 1, 5, 0], inplace = True)\n",
    "    dataset['RoofMatl'].replace(to_replace = ['ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl'], value = [7, 6, 5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['ExterQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa'], value = [3, 2, 1, 0], inplace = True)\n",
    "    dataset['ExterCond'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['BsmtQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['BsmtCond'].replace(to_replace = ['Gd', 'TA', 'Fa', 'Po', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['BsmtExposure'].replace(to_replace = ['Gd', 'Av', 'Mn', 'No', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['BsmtFinType1'].replace(to_replace = ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['BsmtFinType2'].replace(to_replace = ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['HeatingQC'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po'], value = [4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['KitchenQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa'], value = [3, 2, 1, 0], inplace = True)\n",
    "    dataset['Functional'].replace(to_replace = ['Typ', 'Min1', 'Min2', 'Mod',  'Maj1', 'Maj2', 'Sev'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['FireplaceQu'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['GarageType'].replace(to_replace = ['2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['GarageFinish'].replace(to_replace = ['Fin', 'RFn', 'Unf', 'None'], value = [3, 2, 1, 0], inplace = True)\n",
    "    dataset['GarageQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['GarageCond'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa',  'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\n",
    "    dataset['PavedDrive'].replace(to_replace = ['Y', 'P', 'N'], value = [2, 1, 0], inplace = True)\n",
    "    dataset['Fence'].replace(to_replace = ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'None'], value = [4, 3, 2, 1, 0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for cat nominal features\n",
    "train_cat_nominal_one_hot = pd.get_dummies(train[cat_feats_nominal], drop_first= True).reset_index(drop=True)\n",
    "test_cat_nominal_one_hot = pd.get_dummies(test[cat_feats_nominal], drop_first= True).reset_index(drop=True)\n",
    "\n",
    "final_train, final_test = train_cat_nominal_one_hot.align(test_cat_nominal_one_hot, join='inner', axis=1)  # inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all categorical columns and concat with one hot encodings\n",
    "train = train.drop(cat_feats_nominal, axis= 'columns')\n",
    "test = test.drop(cat_feats_nominal, axis='columns')\n",
    "\n",
    "train = pd.concat([train, final_train], axis='columns')\n",
    "test = pd.concat([test, final_test], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-soundtrack",
   "metadata": {},
   "source": [
    "The training dataset is looking good!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-greek",
   "metadata": {},
   "source": [
    "## Target Variable Analysis and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-integration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distplot_probplot():\n",
    "    fig, ax = plt.subplots(1,2, figsize= (15,5))\n",
    "    \n",
    "    #sns.distplot(y_train, label='test_label2', norm_hist='True', ax=ax[0])\n",
    "    sns.distplot(y_train, fit=stats.norm,label='test_label2', ax = ax[0])\n",
    "    stats.probplot(y_train, plot = ax[1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    (mu, sigma) = stats.norm.fit(y_train)\n",
    "    print('mean= {:.2f}, sigma= {:.2f}, mode= {:.2f})'.format(mu, sigma, stats.mode(y_train)[0][0]))\n",
    "    \n",
    "def normality_stats():\n",
    "    print(f\"Skewness: {abs(y_train).skew()}\")\n",
    "    print(f\"Kurtosis: {abs(y_train).kurt()}\")\n",
    "    \n",
    "distplot_probplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-arcade",
   "metadata": {},
   "source": [
    "`SalePrice` is right skewed. We can try log transformation to correct the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(y_train)\n",
    "\n",
    "distplot_probplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-change",
   "metadata": {},
   "source": [
    "The data is finally ready for training!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-location",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,ElasticNet, Lasso, BayesianRidge, Ridge\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlxtend.regressor import  StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "random_state = 42\n",
    "kf = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "secondary-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return (rmse)\n",
    "\n",
    "cv_scores = []\n",
    "cv_std = []\n",
    "\n",
    "#models = ['linear_reg','bayesian_ridge_reg','lasso_reg','elastic_net_reg','ridge_reg','svr_reg', 'gbr_reg', 'lgbm_reg',\n",
    "  #                 'xgb_reg','stacking_cv_reg']\n",
    "\n",
    "\n",
    "def score_model(model_reg):\n",
    "    score_model_reg = cv_rmse(model_reg)\n",
    "    print(f'score_model_reg => mean: {score_model_reg.mean()}, std: {score_model_reg.std()}')\n",
    "    cv_scores.append(score_model_reg.mean())\n",
    "    cv_std.append(score_model_reg.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-temperature",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "novel-omaha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 379823607799.15564, std: 759647068323.0322\n"
     ]
    }
   ],
   "source": [
    "linear_reg = LinearRegression()\n",
    "score_model(linear_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-spring",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "israeli-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 11959062486281.871, std: 23918124825288.492\n"
     ]
    }
   ],
   "source": [
    "bayesian_ridge_reg = BayesianRidge(compute_score= False)\n",
    "score_model(bayesian_ridge_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-cathedral",
   "metadata": {},
   "source": [
    "### Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "rocky-bolivia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vynguyen/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.37462e-46): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/Users/vynguyen/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.39328e-46): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/Users/vynguyen/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.45959e-46): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 24381.20243838968, std: 784.4514847493151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vynguyen/ml/my_env/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.28804e-46): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = pipeline.Pipeline([(\"scaling\", preprocessing.RobustScaler()),\n",
    "                               (\"ridge\", Ridge(random_state= random_state))])\n",
    "\n",
    "score_model(ridge_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-examination",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = pipeline.Pipeline([(\"scaling\", preprocessing.RobustScaler()),\n",
    "                               (\"lasso\", Lasso(alpha=0.0004,\n",
    "                                               max_iter=1e7,\n",
    "                                               tol= 0.001,\n",
    "                                               random_state= random_state))])\n",
    "\n",
    "score_model(lasso_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-cargo",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "external-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 0.36638681187316946, std: 0.011399735102939838\n"
     ]
    }
   ],
   "source": [
    "elastic_net_reg = pipeline.Pipeline([(\"scaling\", preprocessing.RobustScaler()),\n",
    "                               (\"elastic_net\", ElasticNet(positive= True,\n",
    "                                                          precompute=False,\n",
    "                                                          selection='random',\n",
    "                                                          max_iter=1e7,\n",
    "                                                          tol= 0.001,\n",
    "                                                          random_state= random_state))])\n",
    "\n",
    "score_model(elastic_net_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-material",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "activated-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 0.11301981712560838, std: 0.010541678489037408\n"
     ]
    }
   ],
   "source": [
    "svr_reg = pipeline.Pipeline([(\"scaling\", preprocessing.RobustScaler()),\n",
    "                               (\"svr\", svm.SVR(C= 46,\n",
    "                                               epsilon= 0.009,\n",
    "                                               gamma= 0.0003))])\n",
    "\n",
    "score_model(svr_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-numbers",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "constant-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_model_reg => mean: 0.12036832672240023, std: 0.011396707570450363\n"
     ]
    }
   ],
   "source": [
    "gbr_reg = GradientBoostingRegressor(n_estimators=2500,\n",
    "                                      learning_rate= 0.03,\n",
    "                                      random_state = random_state)\n",
    "    \n",
    "score_model(gbr_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-genius",
   "metadata": {},
   "source": [
    "### Stacking CV Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-bearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = ( linear_reg, svr_reg, bayesian_ridge_reg, ridge_reg, lasso_reg, elastic_net_reg, gbr_reg)\n",
    "final_estimator = gbr_reg\n",
    "    \n",
    "stacking_cv_reg = StackingCVRegressor(regressors= estimators,\n",
    "                                  meta_regressor = final_estimator,\n",
    "                                  use_features_in_secondary= True,\n",
    "                                  random_state= random_state)\n",
    "\n",
    "\n",
    "score_model_reg = np.sqrt(-cross_val_score(stacking_cv_reg, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "print(f'score_model_reg => mean: {score_model_reg.mean()}, std: {score_model_reg.std()}')\n",
    "cv_scores.append(score_model_reg.mean())\n",
    "cv_std.append(score_model_reg.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-daniel",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "central-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model):\n",
    "    model.fit(train, y_train)\n",
    "    preds = model.predict(test)\n",
    "    submission = {'Id': test.Id, 'SalePrice': preds}\n",
    "    submission = pd.DataFrame(data=submission)\n",
    "    submission.to_csv(type(model).__name__ + '_submission', index=False)\n",
    "    print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "international-douglas",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 200 features, but DecisionTreeRegressor is expecting 207 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-a5bf6d41ef30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbr_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-07be22a77efe>\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# In regression we can directly return the raw value from the trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[1;32m    626\u001b[0m                        raw_predictions)\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;34m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[1;32m    403\u001b[0m                                     reset=False)\n\u001b[1;32m    404\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/my_env/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[0;31mValueError\u001b[0m: X has 200 features, but DecisionTreeRegressor is expecting 207 features as input."
     ]
    }
   ],
   "source": [
    "submit(gbr_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
